RED NEURONAL 
Una red neuronal es un modelo computacional inspirado en el funcionamiento del cerebro humano. Está formada por nodos o "neuronas" organizados en capas (entrada, ocultas y salida), conectados entre sí. Estas redes aprenden a realizar tareas ajustando los pesos de las conexiones mediante algoritmos como el aprendizaje supervisado, no supervisado o por refuerzo. 
La regresión lineal es una herramienta básica para predecir una variable usando una línea ajustada. Las redes neuronales extienden este concepto al agregar capas y neuronas para modelar relaciones no lineales entre las variables, permitiendo resolver problemas más complejos.
Agregar más neuronas permite que la red modele patrones más complejos y relaciones no lineales en los datos. Sin embargo, demasiadas neuronas pueden llevar al sobreajuste, por lo que debe haber un balance adecuado.
A demás nos permite tener conocimiento jerárquico, permite que las capas iniciales aprendan características simples y que las capas posteriores combinen esas características en patrones más complejos y mejora la capacidad de generalizar, facilitando el análisis de estructuras en datos complejos.
Explica cómo las redes neuronales han evolucionado y la importancia de un algoritmo clave, la retropropagación de errores (backpropagation), para entrenarlas de manera eficiente. Introduce el concepto de redes neuronales y su capacidad para modelar información compleja.
* Describe los inicios con una sola neurona (perceptrón), que era limitada a resolver problemas lineales.
* Explica cómo la investigación en redes neuronales casi desapareció tras las críticas de Minsky y Papert, lo que llevó al "invierno de la inteligencia artificial".
* La publicación de un artículo por Rumelhart, Hinton y Williams presentó un nuevo algoritmo, backpropagation, que permitió entrenar redes neuronales más complejas ajustando automáticamente sus parámetros.
* Retropropagación es una técnica que asigna responsabilidades a las neuronas por el error final de la red, propagando dicho error hacia atrás capa por capa.Este enfoque mejora la eficiencia en comparación con métodos de fuerza bruta que requerían múltiples cálculos para ajustar los parámetros.
* Backpropagation calcula el gradiente necesario para minimizar el error, permitiendo que el descenso del gradiente ajuste los parámetros de la red.
Importancia y eficiencia de la retropropagación en el aprendizaje automático, preparando el terreno para un análisis más matemático en futuros contenidos.
Nos muestra una herramienta interactiva llamada TensorFlow Playground. Esta herramienta permite experimentar con redes neuronales ajustando parámetros, funciones de activación y estructuras de capas para resolver problemas como clasificación y regresión. El objetivo es desarrollar intuición sobre cómo las redes procesan información, transforman datos y separan patrones en el espacio. Además, se ofrecen ejemplos prácticos y retos para profundizar en el aprendizaje.

